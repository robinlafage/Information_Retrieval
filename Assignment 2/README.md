# RI Assignment 2

## How to run the programm

### Install dependencies
```bash
pip install -r requirements.txt
```

### Download the data
```bash
wget -O model_data.tgz "https://www.dropbox.com/scl/fi/ekib1ax383yzt55eqk55w/model_data.tgz?rlkey=t94ex64xsuyrula37a6vszlzd&st=gjcjfthy&dl=0"
tar xvzf model_data.tgz
```

### Run the reranker
```bash
cd src
python3 main.py inputFile outputFile medline [-h] [-m MODEL]
```

#### Arguments
- `inputFile`: Path to the input file to rerank
- `outputFile`: Path to the output file
- `medline`: Path to the medline file

#### Options
- `-h`, `--help`: Show help message and exit
- `-m MODEL`, `--model MODEL`: Model to use for reranking. By default, `../model_data/model.pth` is used.

#### Example
```bash
cd src
python3 main.py ../retrieved_docs_assignment1.jsonl ../output.jsonl ../documents/MEDLINE_2024_Baseline.jsonl
```


### Train the model
```bash
cd src
python3 main.py --train medline training_data outputFile [-h] [-g gloveFile]
```

#### Arguments
- `medline`: Path to the medline file
- `training_data`: Path to the training data file
- `outputFile`: Path to the output file

#### Options
- `-h`, `--help`: Show help message and exit
- `-g gloveFile`, `--gloveFile gloveFile`: Path to the glove file. By default, `../model_data/glove.6B.50d.txt` is used.

#### Example
```bash
cd src
python3 main.py --train ../documents/MEDLINE_2024_Baseline.jsonl ../documents/training_data.jsonl ../model_data/model2.pth
```


## Model

The model is decomposed in multiple important layers : 

#### Embedding layer 

The embedding layer is the layer containing the information about the tokens. In our implementation, this layer is pretrained with GloVe embeddings, and all words not included in GloVe are set with a tensor of 0. This layer is frozen, so it is not modified by the training.

#### Conv2D layer 

The Conv2D layer is the layer that makes the relations between embeddings and that helps to detect the patterns in phrases.

#### MaxPool2D layer 

The MaxPool2D layer is important because it is the layer that reduces the size of the data. It permits to limit to dimensions of the used data (in our case (3,1)) in order to limit the words surroundings used. It simplifies the calculation phase, but does not produce a fixed size data, which is the reason why we use the next layer.

#### AdaptiveMaxPool2D layer 

The AdaptiveMaxPool2D layer permits to be sure we have a fixed size output for the next layer.

#### ReLU layer

The ReLU layer is used to make the model learn more complex relations. 

#### Linear layer

The Linear layer is the layer that makes the various important calculations. In our case it is separated in 2 in order to make a better learning in the first layer, and then in the second layer the data is reduced to only one value, which represents the decision of the model, and which is transformed in a probability with the function sigmoid. 

## Reranking

For a given query, for each document in the list of relevant documents, we tokenize the query and the document to pass the lists of token ids to our model. The model returns a probability, which allows us to rank the documents by relevance.  
To improve efficiency, we send the query and multiple documents to our model at once. 

## Results

With the first model we implemented and trained, the results were very poor. Indeed, using the file generated by the first assignment, we obtained an NDCG@10 of about **0.197**, instead of **0.682** before the reranking phase.  
The problem probably comes from the fact that our lists of token ids are too padded, which leads to a loss of model accuracy.  
Thus, we changed the model implementation to use dynamic padding, which reduces the padding size.  
With this new model, we obtain an NDCG@10 of **0.342**.  
It's much better, but the model does not improve the initial score.

## Possible ways to achieve improvement
A way to make the model better could be by authorizing the improvement of the embedding layer. After some investigation, we discovered that about 1 million of new tokens is created due to the medline's words specificities. This hasn't been tested due to the late discovery of this embeddings addings, which induced a lack of time to train the model.

Another improvement idea could be to change the embeddings layer and use a more powerful one like BERT. By doing this, we could improve the way the words are understood, since bert can make the difference in the meanings of the words (exemple : the word bank as seen in the lectures), but GloVe can not do that.
