# RI Assignment 2

## How to run the programm

### Install dependencies
```bash
pip install -r requirements.txt
```

### Run the reranker
```bash
cd src
python3 main.py inputFile outputFile medline [-h] [-m MODEL]
```

#### Arguments
- `inputFile`: Path to the input file to rerank
- `outputFile`: Path to the output file
- `medline`: Path to the medline file

#### Options
- `-h`, `--help`: Show help message and exit
- `-m MODEL`, `--model MODEL`: Model to use for reranking. By default, `../model_data/model.pth` is used.

#### Example
```bash
cd src
python3 main.py ../documents/retrieved_docs_assignment1.jsonl ../output.jsonl ../documents/MEDLINE_2024_Baseline.jsonl
```


### Train the model
```bash
cd src
python3 main.py --train medline training_data outputFile [-h] [-g gloveFile]
```

#### Arguments
- `medline`: Path to the medline file
- `training_data`: Path to the training data file
- `outputFile`: Path to the output file

#### Options
- `-h`, `--help`: Show help message and exit
- `-g gloveFile`, `--gloveFile gloveFile`: Path to the glove file. By default, `../model_data/glove.6B.50d.txt` is used.

#### Example
```bash
cd src
python3 main.py --train ../documents/MEDLINE_2024_Baseline.jsonl ../documents/training_data.jsonl ../model_data/model2.pth
```


## Model

TODO

## Reranking

For a given query, for each document in the list of relevant documents, we tokenize the query and the document to pass the lists of token ids to our model. The model returns a probability, which allows us to rank the documents by relevance.  
To improve efficiency, we send the query and multiple documents to our model at once. 

## Results

With the first model we implemented and trained, the results were very poor. Indeed, using the file generated by the first assignment, we obtained an NDCG@10 of about 0.197, instead of 0.682 before the reranking phase.  
The problem probably comes from the fact that our lists of token ids are too padded, which leads to a loss of model accuracy.  
Thus, we changed the model implementation to use dynamic padding, which reduces the padding size. (Verify this last sentence)  
With this new model, we obtain an NDCG@10 of 0.342.  
It's much better, but we still can't improve the initial score.